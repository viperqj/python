解决csv文件乱码  f=open('./豆瓣.csv','w',encoding='utf-8-sig',newline='')
./表示当前位置
爬虫在使用场景中的分类：
    一，通用爬虫：
      抓取系统重要组成部分。抓取的是一整张页面数据。
    二，聚焦爬虫：
      建立在通用爬虫的基础上，抓取的是页面中特定的局部内容。
    三，增量式爬虫：
      检测网站中数据更新的情况。只会抓取网站中最新更新出的数据。
爬虫的矛与盾
反爬机制
    门户网站，可以通过制定相应的策略或技术手段，防止爬虫程序进行网站数据的爬虫。
反反爬策略
    爬虫程序通过制定相关的策略或手段破解门户网站中具备的反爬机制

robots.txt协议：
    君子协议。规定了网站中哪些数据可以被爬取哪些数据不可以被爬取。

http协议：
    就是服务器和客户端进行数据交互的一种形式。
常用请求头信息：
    User-Agent:请求载体的身份标识
    Connection：请求完毕后，是断开还是保持连接

常用响应头信息：
    Content-Type：服务器响应回客户端的数据类型

https协议：
    安全的超文本传输协议

加密方式
    对称秘钥加密
    非对称秘钥加密
    证书秘钥加密

聚焦爬虫：爬取页面中指定的页面内容：
    -流程
    1.指定url
    2.发起请求
    3.获取响应数据
    4.数据解析
    5.持久化存储

数据解析分类：
    -正则
    -bs4
    -xpath(***)
数据解析原理概述：
    -解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储。
    -1.进行指定标签的定位
    -2.标签或者标签对应的属性中存储的数据进行提取(解析)

<li><a href="/meinv/20201110/219859.html" target="_blank"><img src="https://img.lianzhixiu.com/uploads/allimg/202011/9999/rna083e11602.jpg" alt="浴巾美女香肩美腿真空诱惑美图" border="0"></a>
<span class="soxflashtext"><a href="/meinv/20201110/219859.html" target="_blank">浴巾美女香肩美腿真空诱惑美图</a></span>
</li>
提取 src
ex = '<li>.*?<img src="(.*?)" alt=.*?</li>'
img_scr_list = re.findall(ex,page_text,re.S)
print(img_scr_list)
66666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666
ex = '<div class="thumb">.*?<img src="(.*?)" alt=.*?</div>'
img_scr_list = re.findall(ex,page_text,re.S)
for src in img_scr_list:
    #拼接出一个完整的图片地址
    src = 'https:'+src
    #请求到图片的二进制数据
    img_data = requests.get(url=src,headers=headers).content
#生成图片名称
    img_name = src.split('/')【-1】
    # 图片路径加名称,即图片最终存储的路径
    imgpath = './qiutuLibs/' + img_name
    with open(imgpath, 'wb') as fp:
        fp.write(img_data)
        print(img_name, '下载成功')

bs4进行数据解析
    -数据解析的原理
        -1.标签定位
        -2.提取标签中存储的数据值
     bs4数据解析的原理:
         -1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
         -2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取
-环境安装：
    -pip install bs4
    -pip install lxml
-如何实例化对象：
    -1.将本地的html文档加载到该对象中
        fp=open('./test.html,'r',encoding='utf-8')
        soup=BeautifulSoup(fp,'lxml')
    -2.将互联网获取的页面源码加载到该对象中
        page_text = response.text
        soup =BeautifulSoup(page_text,'lxml')
！！！！！！!!提供数据解析的方法和属性：！！！！！！！！！！！！！！！！！11
        -soup.tagName:返回的是文档中第一次出现tagNam对应的标签
        -soup.find():
            1.#find('tagname'):===soup.tagname
            2.属性定位
            print(soup.find('a',href_=""))后边加属性定位
        -soup.find_all('tagName'):返回符合要求的所有标签(列表)
    select：
            -select('某种选择器(id,class,标签。。。选择器）')返回的是一个列表
            -层级选择器：
                —soup.select('.tang>ul>li>a'):>表示的是一个层级
                -soup.select('.tang>ul a')：空格表示多个层级
    ——获取标签之间的文本数据：
        -soup.a.text/string/get_text()
        -text/get_text()可获取标签下所有文本内容
        -string:只可以获取该标签下面直系文本内容
    -获取标签中属性值：
        -soup.a['href']
xpath解析:最常用最高效的一种解析方式。通用性 返回的是列表
    - xpath解析原理:
        -1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中
            -本地：parser=etree.HTMLParser(encoding='utf-8')
                      tree=etree.parse('test.html',parser=parser)
            -网上：tree=etree.HTML(page_text)
        -2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
       -环境的安装:
        -pip install lxml
    -如何实例化一个etree对象:from lxml import etree
        -1.将本地的html文档中的源码数据加载到etree对象中:
                etree.parse(filePath)
        -2.可以将从互联网上获取的源码数据加载到该对象中
                 etree. HTML('page text')
        -xpath('xpath表达式')
    -xpath表达式：
        -/:表示从根节点开始定位。表示的是一个层级
        -//表示的是多个层级 可以表示从任意位置开始定位
        -属性定位：//div[@class=" "] tag[@attname="属性值"]
        -索引定位：// div[@class=" "]/p[3]
        -取文本:/text()获取的是直系的文本内容 //text()标签中非直系文本内容
        -取属性:/@attrName   a/@href



